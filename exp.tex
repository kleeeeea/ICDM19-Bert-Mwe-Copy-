%!TEX root =  concept_mining.tex


\section{Experiments}
\label{sec:experiments}

\begin{table}
\center
\begin{tabular}{ >{\centering\arraybackslash}m{1.5cm} || >{\centering\arraybackslash}m{1.5cm} | >{\centering\arraybackslash}m{1.5cm} | >{\centering\arraybackslash}m{ 1.5cm}  }
\hline
\textbf{Data} & $\# labels$ & \# words &  \# docs  \\ \hline
Quora & $2$ & $10M$ & $404K$  \\ 
Aminer &  $14$ & $25M $ &  $157K$  \\ 
DBPedia &  $15$ & $31M$ &  $630K$  \\ 
Arxiv CS & N/A & $25M$ & $139K$ \\
Pubmed & N/A & $16M$ & $110K$ \\ 
 \hline
\end{tabular}
\caption{Dataset statistics}\label{tab-data-stats}
\end{table}

\subsection{Setup}In this section, we evaluated the performance of our n-gram embedding approach over a wide range of NLP tasks: predicting label from sequence, predicting similarity between pair of sequences, and predicting the sequence itself. For each task, we selected two representative neural network architecture: one based on convolutional neural network, and one based on recurrent neural network. For the network architectures based on convolutional neural network, we apply the n-gram embedding with two strategies: using it as word replacement, and using it as pretrained local feature detector. For the recurrent neural network based network architectures, since there it did not explicitly model local feature detectors, we apply the  n-gram embedding only by using it as word replacement.

\noindent \textbf{Evaluation Methodology}
Since each model in each specific tasks already have well defined evaluation metrics, we directly use these original metrics and do not define our own. 
Since we mainly care about the \emph{difference} our approach brings to the model, we use the following procedure of evaluation: 
we first tune the original model to a satisfactory performance, mostly based on the original settings of the model, \footnote{all the code, and hyperparameter settings will be released to public}
and then by keeping all the model hyper-parameters \emph{exactly the same}, we apply our n-gram embedding approach to the model and measure the performance using the same evaluation metrics.




\noindent \textbf{Hardware}
We ran our experiment over a single GeForce GTX 1080 GPU card with 12G memory, on a standard server with 32G memory and 12 cores. The longest experiments took less than 8h to finish.

\subsection{Dataset}
We have collected the following 5 dataset, with the hope that the text contains rich multiword expressions for us to evaluate our n-gram embedding approaches. The statistics is shown in \autoref{tab-data-stats}. 


\noindent \textbf{Quora} We use the publicly available Quora duplicate question detection dataset \footnote{quora.com}. Each data point is a pair of plain text questions, as well as a label denoting whether they are duplicate questions, and the goal is to predict this label.

\noindent \textbf{DBPedia}
We use the DBPedia ontology dataset, release by \cite{zhang2015character}. 
It consists of 14 nonoverlapping class of entities in DBPedia, each is associated with a textual description as well as the class it belongs to, and the goal is to predict the class using the text.

\noindent \textbf{Aminer}
We collected another dataset for the classification task, by picking 14 nonoverlapping categories of computer science papers from the complete paper collection from academic search engine \cite{Tang08KDD}. Each paper is associated with title and abstract, and the goal is to predict its category.


\noindent \textbf{Arxiv CS}
We collected all the papers from arxiv.org under the category of computer science from the year of 2008 to 2018. Each paper is associated with its title and abstract. 

\noindent \textbf{Pubmed}
We construct a text collection in the medicine domain, 
select the 31 publication sources 
from the Pubmed paper abstract collection \footnote{https://www.ncbi.nlm.nih.gov/pubmed/}
that has more than 1M  bytes of words and SCI impact factor above 1. Each paper is associated with title and abstract.


\subsection{Results}
We presented our experimental results below, divided by the specific tasks: predicting similarity between pair of sequences, i.e. text similarity, 
predicting label from sequence, i.e. text classification, 
and predicting the sequence itself, i.e. language modelling. 
All the code, pre-processed data and hyper-parameter configurations will be released to public.



\noindent \textbf{Predicting the similarity between pair of sequences}
We evaluate our approach over the task of predicting the similarity between pair of sequences, using the Quora dataset. The result is shown in \autoref{tab-seq-pair}, where the performance is measured using the cross entropy loss as well as prediction accuracy. Again the n-gram embedding bring benefits to the model, for example, adding n-gram embedding as word replacement to CNN architecture reduce the loss by 5.7\%.

\noindent \textbf{Predicting label from sequence}
The results for different datasets and different models are shown in \autoref{tab-seq-to-label}. For each task, we measure its performance via the cross entropy loss, as well as the prediction accuracy. We can see that adding our n-gram approach brings positive benefit to different architectures, according to both metric, with the improvement in loss function more evident. For example, adding n-gram embedding to the LSTM model reduce the loss by 20\%, and 7\%, for each respective dataset.


\begin{table*}
\vspace{15pt}
\caption{Results of N-gram embedding on Text Classification}
\label{tab-seq-to-label}

\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lllll}  
\toprule
& \multicolumn{2}{c}{ \thead{Aminer} } & \multicolumn{2}{c}{ \thead{DBPedia} } \\
\thead{model} &  \thead{loss ($\times 10^{-2} $) } & \thead{accuracy(\%)}  & \thead{loss} ($\times 10^{-4} $)  & \thead{accuracy(\%)}  \tabularnewline
\midrule
CNN \cite{kim2014convolutional} \\
original      &  1.704   & 64.02    & 8.07     & 98.65  \\
original + word replacement         &    1.695  (-0.5\%)   &     64.10 (+0.1\%) & 7.93 (-1.7\%)   & 98.66 (+0.01\%)  \\
original + pretrained local feature      & 1.688 (-0.9\%)   &  64.78 (+1.2\%)  & 8.06  (-0.1\%)   & 98.70 (+0.05\%)   \\
LSTM \cite{zhou2016attention}\\
original       & 2.779     &   58.58  & 6.24     & 98.94    \\
original + word replacement    &   2.242  (-20\%) &   60.14  & 5.84 (-6.4\%)    & 99.03 (+0.1\%)  \\
\bottomrule
\end{tabular}
}

\vspace{-15pt}
\end{table*}


\noindent \textbf{Predicting the sequence itself}
We apply our n-gram embedding  approach to task of predicting sequences itself, using the Arxiv CS and Pubmed dataset. To evaluate the result of n-gram embedding modification strategy, we only keep the data instance when there exists at least 1 free base entity in the text. The results are shown in \autoref{tab-seq-modeling}. We can see that adding the n-gram embedding brings clear benefit to the CNN model, reducing its perplexity on both datasets by a large margin. The results on the LSTM model varies from dataset: the loss on Arxiv CS is reduced by 10\%, but the loss on Pubmed is not reduced much, and even worse though, is that the accuracy comes down from 0.08\% to 0.06\%. 
 We attribute this to the fact that plain LSTM without attention may not fully enjoy the additional information that n-gram embedding brings to the network, and these extremely low accuracies may not be fully capture the performance of the model.


\begin{table*}
\vspace{15pt}
\caption{Results of N-gram embedding on Language Modeling}
\label{tab-seq-modeling}

\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lllll}  
\toprule
& \multicolumn{2}{c}{ \thead{Arxiv CS} } & \multicolumn{2}{c}{ \thead{Pubmed} } \\
\thead{model} &  \thead{perplexity} & \thead{accuracy(\%)}  & \thead{perplexity}   & \thead{accuracy(\%)}  \tabularnewline
\midrule
CNN \cite{dauphin2016language} \\
original      &  189.56   & 20.21     & 199.1   & 19.34   \\
original + word replacement         &    175.16 (-7.6\%)   &    20.32 (+0.5\%) & 172.3 (-13\%) & 21.10 (+9.1\%) \\
original + pretrained local feature      & 175.04 (-7.7\%)     &  20.74   (+2.6\%) & 179.4 (-9.9\%)     &20.74  (+7.2\%)  \\
LSTM \cite{grave2016efficient}\\
original       & 102.53    &   0.06   & 83.80   & 0.08   \\
original + word replacement    &   99.23  (-9.7\%) &   0.09 (+50\%)  & 83.79  (-0.01\%)   & 0.06   (-25\%) \\
\bottomrule
\end{tabular}
}

\vspace{-15pt}
\end{table*}


We compare the proposed vocabulary selection algorithm against several strong baselines on a wide range of text classification tasks and datasets. 

baseline: bert
bert fine tune 3

vary vocab size
vary no vb

\subsection{sequence modeling}


\noindent \textbf{general domain}
already pretrained
use a subset of wikipedia


\noindent \textbf{technical domain}
fine tune on a 

\subsection{sequence prediction}



In this section, we evaluate the proposed methods with extensive experiments across several technical domains and demonstrate its efficiency and effectiveness. 

\begin{table}
\centering
\caption{Dataset Statistics}\label{tab:dataset-stats}
\vspace{5pt}
\resizebox{.5\textwidth}{!}{%

% in the respective field, including the complete collection of NIPS, JMLR, VLDB, SIGMOD and Pubmed. The details are described below.

\begin{tabular}{cccc}
\toprule
  & \textbf{Computer Science} & \textbf{Physics \& Math} & \textbf{Medicine} \\
\midrule
\# docs & 47K & 127K & 55K \\

\# words &  9M & 22M & 9M \\
taxonomy size & 31 & 28 & 17 \\
taxonomy height & 3 & 5 & 3 \\
\bottomrule
\end{tabular}

}
%\subcaption{JMLR+VLDB}\label{fig:1a}
\end{table}




\begin{table*}[!thbp]
\caption{Overall performance comparison measured by prediction accuracy. The top-K prediction score among all possible taxonomy nodes are recorded and compared against the ground truth according to both flat classification accuracy and taxonomy tree based accuracy}
\label{tbl:entity4ranking}
\centering
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\vspace{10pt}
\scalebox{0.9}{
\begin{tabular}{|c|c||  c|c|c| c|c|c| c|c|c| c|c|c| }
\cline{1-14}
 &  & \multicolumn{2}{c|}{ WeSHClass}   & \multicolumn{2}{c|}{ Pretrain BERT }  & \multicolumn{2}{c|}{ Hier-SVM } & \multicolumn{2}{c|}{ Dataless} & \multicolumn{2}{c|}{ UNEC } & \multicolumn{2}{c|}{ Hiercon } \\
 \cline{1-14}
Dataset & Accuracy Measure &  
\tabincell{c}{\emph{Flat}} &  \tabincell{c}{\emph{Tree}} &  \tabincell{c}{\emph{Flat}} & \tabincell{c}{\emph{Tree}} &  \tabincell{c}{\emph{Flat}} &  \tabincell{c}{\emph{Tree}} &
\tabincell{c}{\emph{Flat}} & \tabincell{c}{\emph{Tree}} &  \tabincell{c}{\emph{Flat}} &  \tabincell{c}{\emph{Tree}} &
\tabincell{c}{\emph{Flat}} &  \tabincell{c}{\emph{Tree}} \\
\hline
\hline
\multirow{2}{*}{\makecell{\textsf{Computer Science}}} 
&  Top 1 &  0.3082 & 0.4453 & 0.4354 & 0.6207 & 0.1095 & 0.4156 & 0.2094 & 0.5453 & 0.1742 & 0.3502 & 0.7927 & 0.8763 \\


\cline{2-14}
&  Top 3 & 0.4984 & 0.6672 & 0.6665 & 0.8676 & 0.2894 & 0.4747 & - & - & 0.3405 & 0.6633 & 0.9486 & 0.9613 \\


\cline{2-14}
&  top 5 & 0.6004 & 0.7870 & 0.7728 & 0.9305 & 0.3685 & 0.6758 & - & - & 0.4358 & 0.8072 & 0.9803 & 0.9846 \\


\cline{2-14}
\cline{1-1}
\hline
\hline
\multirow{2}{*}{\makecell{\textsf{Physics \& Mathematics}}}  
&  Top 1 & 0.3551 & 0.7100 & 0.3292 & 0.7262 & 0.0508 & 0.0555 & 0.2403 & 0.6229 & 0.2507 & 0.4149 & 0.7223 & 0.9270 \\


\cline{2-14}
&  Top 3 & 0.6086 & 0.8692 & 0.6355 & 0.8450 & 0.0605 & 0.0605 & - & - & 0.4939 & 0.7329 & 0.9450 & 0.9791 \\


\cline{2-14}
&  top 5 & 0.7292 & 0.9233 & 0.7985 & 0.9061 & 0.0608 & 0.0710 & - & - & 0.6157 & 0.8444 & 0.9787 & 0.9871 \\


\cline{2-14}
\cline{1-1}
\hline
\hline
\multirow{2}{*}{\makecell{\textsf{Medicine}}}  
&  Top 1 & 0.2478 & 0.7208 & 0.3550 & 0.7943 & 0.0000 & 0.5491 & 0.1641 & 0.6193 & 0.3275 & 0.5130 & 0.5296 & 0.8375 \\


\cline{2-14}
&  Top 3 & 0.5562 & 0.8563 & 0.6775 & 0.9394 & 0.0512 & 0.7500 & - & - & 0.5932 & 0.8347 & 0.8399 & 0.9146 \\



\cline{2-14}
&  top 5 & 0.7249 & 0.9567 & 0.8145 & 0.9805 & 0.3661 & 0.7903 & - & - & 0.7361 & 0.9256 & 0.9437 & 0.9864


\cline{2-14}
\cline{1-1}
\hline
\hline
\multirow{1}{*}{\makecell{\textsf{Average (Top 1)}}}  
&  - & 0.3037 &  0.6254 &  0.3732 &  0.7137 &  0.0534 &  0.3401 &  0.2046 &  0.5958 &  0.2865 &  0.5331 &  0.6815 &  0.8803
 \\

\hline
\end{tabular}
}
\vspace{-5pt}
\label{table-overall}
\end{table*}

\begin{figure*}[h!]
 \includegraphics[width=\textwidth]{fig/detailed_performance.pdf}
 \caption{Detailed Performance evaluation divided by the subject categories. The flat classification accuracy for each sub-fields are recorded top-K prediction score are displayed along specific vertical axis. 
 %\Yu{Move the legend to the top as a row. It's blocking.}
 }
 \vspace{-10pt}
\label{fig-performance-evaluation-by-category}
\end{figure*}


\subsection{Experiment settings}
\subsubsection{Computing environment}
%All experiments are conducted on a conducted on a gpu
%all the experiments finishes in less than 2 hours.
All the model training and evaluation pipeline are conducted on a lab server with with 3 GeForce RTX 2080 GPU card and 2 6-core Intel(R) Core(TM) i7-6800K CPU @ 3.40GHz CPU with 12GB memory. 
The longest run took less than 1 hour. 



\subsubsection{Datasets}
%The datasets for computer science and physics \& mathematics are from the open access repository \textit{arxiv.org}, and the datasets for medicine are collected from another open access repository, \textit{pubmed}.
We have collected the following corpus, covering the domain of computer science, physics \& mathematics and medicine.
The statistics of these datasets are summarized in Table S1, and the details are described below.

\noindent \textbf{Computer Science} 
The Computer Science corpus is obtained by crawling paper abstracts from \url{arXiv.org} under the top level category "computer science", and aligning the arxiv's category with aminer.org's academic conference classification\footnote{\url{https://aminer.org/ranks/conf}}.

\noindent \textbf{Physics \& Mathematics} 
The Physics \& Mathematics corpus is also obtained from \url{arXiv.org}, 
by crawling paper abstracts under the top level category "physics" and "mathematics", and aligning them with the math subject headings\footnote{\url{https://www.maa.org/press/periodicals/loci/joma/subject-taxonomy}}
and the physics subject headings\footnote{\url{https://physh.aps.org/}},
\nop{
which results in a height 5 taxonomy tree of 80 nodes. 
We selected 573K documents that are only associated one primary category in the taxonomy tree as our corpus, which we will be able to use as automatically labeled test data.
}
\nop{
There are 67 categories in total, and we manually group them into an hierarchy according to 
This results in X documents, a height 5 taxonomy with 50 nodes total.
}

\noindent \textbf{Medicine}
The Medicine corpus is obtained by crawling abstracts from Pubmed\footnote{\url{https://www.ncbi.nlm.nih.gov/pubmed/}}, and we directly take the top 3 level of 
the Medicine subject headings\footnote{\url{https://www.nlm.nih.gov/mesh/meshhome.html}} under the top level category "Organisms", "Analytical, Diagnostic and Therapeutic Techniques, and Equipment" and "Psychiatry and Psychology" as the taxonomy.



\subsubsection{Compared methods}
We compare our method with a wide range of state-of-the-art ones, as described below:

\noindent \textbf{WeSHClass} \cite{meng2018weakly} is a state of the art approach for weakly supervised hierarchical classification. %\SY{Not clear >} 
It first generates a set of pseudo-documents for each class based on supervised signal such as labeled documents to train the classification model, then bootstrap on unlabeled data. We follow the author's  implementation\footnote{\url{https://github.com/yumeng5/WeSHClass/}} and use their recommended settings.



\noindent \textbf{Dataless} refers to the hierarchical dataless classifcation approach, which is \cite{song2014dataless} is a state of the art distant supervision based method in the hierarchical classification settings.
It works by first obtaining vector representations of documents and class labels
by its similarity with Wikipedia articles \cite{egozi2011concept}, and 
associate documents based on the class labels based on its vector representation.  
We follow the original dataless implementation classification from the authors\footnote{\url{https://cogcomp.org/page/download_view/Descartes}}, and apply it with a bottom-up scheme.


\noindent \textbf{Pretrain BERT} is the currently the state of the art deep learning approach for downstream NLP tasks, which leverage very deep self-attention based architecture with weights pretrained on large training corpus. Specifically, we add an classification layer added upon the first input  token ([CLS]) from the last transformer layer, same as the experiment used for GLUE  classification tasks  \cite{devlin2018bert}, fine-tune all the layers and report the test accuracy under the best hyper-parameter settings.


\noindent \textbf{Hierarchical SVM} \cite{liu2005support} augments SVM classifier with hierarchical information with a top-down paradigm \cite{silla2011survey}, where training documents for child nodes are included in their ancestors.

\noindent \textbf{UNEC} \cite{li2018unsupervised} is a state of the art unsupervised text categorization method that extracts concepts by segmenting the corpus into phrases and then learns a concept embedding graph, where similarity to classes are propagated. We adopt an alternative, personalized page rank approach\footnote{\url{https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html}} to propagate the similarity on the concept graph.


\noindent \textbf{Hiercon} refers to our proposed approach. %\SY{Too long, break this sentence >} 
We utilize the concept mining technique ECON \cite{li2018concept} to obtain concepts for representing documents and categories.
The concept embedding vector used for similarity computation is trained using the Skip-gram objective as a 50 dimensional embedding vector for each concept.
And in the dynamic bin pooling stage, we divide the concept similarities into $K=16$ bins, 15 of which equally spaced bins between $[-0.5,1)$ plus 1 for exact match, to be used in bin pooling for the similarity aggregation.



\subsubsection{Evaluation methodology}
In order to evaluate the above methods in a weakly supervised setting,
we randomly select 3 documents for each class label and combine them together as the training set, 
following previous work \cite{meng2018weakly}, and we use all the remaining documents as the test set.

%\SY{Reword >} 
Because the category label from each class may contain noise, 
and that each document may be associated with multiple labels, we adopt the top $k = 1,3,5$ accuracy measure \cite{lapin2015top} to evaluate the performance of each method.

In order to further incorporate tree structure into the evaluation and account for the fact that the prediction into nodes which are closer to the ground truth nodes are less "wrong", we propose a \textit{tree} version of top $k$ accuracy, in addition to the classical \textit{flat} one. Its calculation is based on the following procedure: instead of returning 1 only when the prediction is the same as the ground truth and 0 otherwise, we also return 1 if the prediction is the sibling or the parent node of the ground truth node, and 0 if it is in other part of the tree.

%\vspace{-10pt}

\subsection{Overall Performance with Automatic Evaluation}
\noindent \textbf{Overall performance}
We present the results of the overall performance evaluation in \autoref{table-overall}. Hiercon achieves the best performance overall by effectively deriving concept representation for documents and hierarchy labels, 
and comprehensively utilizing all the concept similarity,
and allowing them to flow to downstream relevance computation.
Pretrained BERT also works relatively well, which confirms the validity of training label, and the generalizability of pre-trained weights; 
WeSHClass is able to improve it by efficiently utilizing training signals.
Dataless is able to achieve coarse level categorization, as seen 
by the tree accuracy, but in general suffers bad performance when  the knowledge base coverage is low.

\noindent \textbf{Performance evaluation for each category}
We show in \autoref{fig-performance-evaluation-by-category} the detailed performance for more specific categories. For better illustration, we group these them into sub-categories for each subject field. 
We can observe that although the performance of each method varies by category depending on the difficulty of the corresponding science field, 
the relative trend stays similar: 
Hiercon is almost always outperform other baseline approaches, with its top 3 prediction covering the right class most of the time, followed by other methods such as WeSHClass. 
There are very few cases where Hiercon doesn't perform well, for example, the category "Logic" in the Physics and Mathematics dataset, possibly due to less accurate concept representation for that category. 
However, in practice we can remedy this by 
associating categories with more accurate concepts to describe the content of that class.




\noindent \textbf{
Performance evaluation with varying numbers of training examples
}
To further investigate how well our model utlize the training data, we perform an ablation study over the number of training instances given to the model. The results are shown in \autoref{fig-num-training-example}, where we give the model various number of training examples and record its top 1 accuracy relative to the original model with the complete training set, as its relative performance measure. We can clearly see that our approach can efficiently fine-tune the model weights with very little number of examples, with 10 training examples, it already reaches a considerable level of accuracy; with 20-30 training examples, the model can approximately recover performance of the original model trained on the full training set.


\subsection{Qualitative Study}






\begin{table}
\centering

\resizebox{\columnwidth}{!}{%
\begin{tabular}{c|c|c} \hline
\multirow{2}{*}{} & \multicolumn{2}{c}{Concepts} \\ \cline{2-3} & discriminative & indiscriminative \\ \hline
\multirow{3}{*}{\it{\textbf{Computer Science}}} & \it{fpga (Hardware)} & \it{framework} \\ & \it{nash 
equilibria (Game Theory)} & \it{goal} \\ & \it{attacks (Security)} & \it{technique} \\ \hline
\multirow{3}{*}{\it{\textbf{Physics & Maths}}} & \it{entanglement (Quantum Physics)} & \it{correspondance} \\ & \it{Jupyter (Planet astrophysics)} & \it{metric} \\ & \it{complete graph  (Combinatorics)} & \it{series} \\
\hline
\multirow{3}{*}{\it{\textbf{Medicine}}} & \it{MRI (Diagnosis)} & \it{levels} \\ & \it{treatment (Therapeutics)} & \it{ability} \\ &  \it{HIV (Viruses)} & \it{regression} \\ \hline
\end{tabular}
}
\vspace{3pt}
\caption{Example discriminative vs indiscriminative concepts discovered in each dataset}
\vspace{-10pt}
\label{table-top-concepts}
\end{table}


\noindent \textbf{Discriminative \& indiscriminative concepts}
If we treat each concept as a document and perform classification on it, we can obtain the direct relevance between concepts and taxonomy nodes. \autoref{table-top-concepts}
shows some of the most discriminative \& indiscriminative concepts. We can see that the concept representation of taxonomy nodes is able to capture latent semantics and make meaningful distinctions on the concept level.


\begin{figure}[]
\centering
\includegraphics[width=.4\textwidth]{fig/num_training_examples_vs_relative_performance.png}
\vspace{10pt}
\caption{Ablation study of \Hiercon over the number of training examples. The performance are measured by the relative (top-1) accuracy of the original performance.}
\label{fig-num-training-example}
\vspace{-3pt}
\end{figure}


\begin{figure}[!ht]
  \begin{subfigure}[c]{.4\textwidth}
                \centering
                \includegraphics[width=\textwidth]{fig/err-db-theory.png}
                \caption{Database confused as Logic (Computer Scicence)}
                %\vspace{-1pt}
                \label{fig-error-case-concept-attention-theory}
  \end{subfigure}\hfill
  
  \begin{subfigure}[c]{.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/err-db-hardware.png}
        \caption{Database confused as Hardware & Architecture}
        \vspace{-10pt}
        \label{fig-error-case-concept-attention-hardware}
  \end{subfigure}
\label{fig-error-case-concept-attention}
\vspace{15pt}
\caption{Error case analysis with concept attention. The attention weights  distribution over different concepts in a document are illustrated with the colors' color along specific columns.
}
\vspace{-10pt}
\end{figure}

\noindent \textbf{Error case analysis with concept attention}
Document's relevance to taxonomy nodes can be viewed as a combination of its individual concepts' relevance, weighted by the attention. 
%\SY{Reword}
\autoref{fig-performance-evaluation-by-category} utilizes this to perform error cases analysis, where concept's attention-weighted relevance to the top-5 predicted category is visualized.
From the figure we can clearly see what concepts and how much they contribute to the final prediction. 
%\SY{Too long, break it in 2-3 sentences.}
For example, in \autoref{fig-error-case-concept-attention-theory} concepts such as "relational algebra" and "decomposition trees" confuse the model to associate the document with more theoretic subject such as logic (coded "cs.LO") and formal languages (coded "cs.FL").
At the meantime, other evidence such as "tpc-h" and "probabilistic database" support the model to partially correctly predict it as database.
Similarly, in \autoref{fig-error-case-concept-attention-hardware} concepts such as "grid computing" and "load balancing" strongly support the model to predict the document as "Hardware \& Architecture" (coded "cs.AR").






