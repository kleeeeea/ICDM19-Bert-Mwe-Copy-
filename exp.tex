%!TEX root =  concept_mining.tex


\section{Experiments}

In this section we experimentally evaluate the proposed approach over an wide range of tasks across different domains and demonstrate its effectiveness over the state of the art baseline approaches.

\label{sec:experiments}


\begin{table*}
\small
\centering
\begin{tabular}{llllll} 
\toprule
Datasets & Task                                                                                               & Description                              & \#Class & \#Train & \#Test  \\
\midrule
ATIS-flight~\cite{tur2010left} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}NLU\end{tabular}}  & Classify Airline Travel dialog           & 21      & 4,478   & 893     \\
Snips~\cite{DBLP:journals/corr/abs-1805-10190}       &                         & Classify inputs to personal voice assistant & 7       & 13,084  & 700     \\
\midrule
AG-news~\cite{zhang2015character}     &  \multirow{4}{*}{\begin{tabular}[c]{@{}l@{}}DC\end{tabular}}  & Categories: World, Sports, etc  & 4 & 120,000 & 7,600   \\
DBPedia~\cite{lehmann2015dbpedia}     &                      & Categories: Company, Athlete, Album, etc & 14      & 560,000 & 70,000  \\
Sogou-news~\cite{zhang2015character}  &                         & Categories: Sports, Technology, etc      & 5       & 450,000 & 60,000  \\
Yelp-review~\cite{zhang2015character} &                         & Categories: Review Ratings (1-5)         & 5       & 650,000 & 50,000  \\ 
\midrule
SNLI~\cite{DBLP:conf/emnlp/BowmanAPM15}        &  \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}NLI\end{tabular}}                   & Entailment: Contradict, Neutral,Entail   & 3       & 550,152 & 10,000  \\
MNLI~\cite{DBLP:conf/naacl/WilliamsNB18}        & & Multi-Genre Entailment   & 3       & 392,702 & 10,000  \\
\bottomrule
\end{tabular}
\caption{An overview of different datasets under different classification tasks including description and sizes.}
\label{tab:dataset}
\end{table*}

\subsection{Setup}

\noindent \textbf{Hardware Environment}
The model training and preprocessing is conducted on a lab server with 3 
GeForce RTX 2080 GPU card, 2 6-core Intel(R) Core(TM) i7-6800K CPU @ 3.40GHz CPU with 12GB memory. The longest experiments took no more than 3 days.


\noindent \textbf{Model architecture and hyper-parameter settings}
Since the proposed \BertMWE approach is built upon the Bert self-attention network architecture \cite{devlin2018bert},
and 
we're mainly interested in how much it can \textit{improve} upon the basic Bert model, 
we follow the exact same architecture, hyper-parameter and optimizer settings as suggested by 
\cite{devlin2018bert} for each specific tasks 
refer the reader to the original paper for details about the architecture and parameter.

\noindent \textbf{Model training and weight initialization}
In order to more directly see the difference between the \BertMWE and the basic case Bert model, we will conduct the model training of \BertMWE in an \textit{incremental} fashion:
we follow the best training settings for each different variant of the Bert model  to train the basic model for a specific downstream task; after that, we start to train the corresponding \BertMWE 
for the Bert variant, and use the trained weighted of base Bert model for that task to initialize the \BertMWE model. In other words, the pre-trained weights of \BertMWE come from the corresponding Bert model without MWE.

\noindent \textbf{Model variants}
we apply the \BertMWE approach over the two different variants of the original Bert as proposed in \cite{devlin2018bert}: the Bert-Finetune as described in Section 3.5 of \cite{devlin2018bert}, where we use the pretrained weights as initialization, and fine-tune the model end to end, and the Bert-Feature, where we freeze the pretrained the model weights and use the outputted hidden states as feature to train a specific classification head. In addition, we evaluate two different versions of apply the MWE, the first, \BertConcat, simply follow the concatenation approach as proposed in \autoref{sec:arch-overview}, without inference over the dropout probabilities over different types of MWE, while the second, \BertMWE, follows the variational inference procedure over the dropout probability procedures and perform posterior predictive distribution for prediciting unseen outputs, as shown in \autoref{sec:var-param}, \autoref{sec:approximate-inference}, \autoref{sec:prediction}.


\subsection{Text modeling} \label{sec:dataset}
We fisrt evaluate the proposed approach over the task of 
learning to modeling the text data itself, without any additional supervision signal.
We follow the original pretraining procedure for finetuning and evaluating different approaches,
by first pre-processing the input corpus for the masked language model objective, and using the the sum of the mean masked LM likelihood and mean next sentence prediction likelihood as the objective function. 
In order to have a fair comparison for the MWE based approaches, we prevent the information "leakage" from the MWE in the sentence that may contain the masked tokens, by discarding in the masking stage any MWE in the input if there is any overlap between the MWE and the masked tokens.


across a wide range of tasks, which follows two paradigm: 
, and learning to predict downstream supervised label from the input text. 



We evaluate various aspects of the text modeling capabilities of the proposed approaches over the following domains, as detailed below. The dataset statistics is shown in \autoref{tab-data-stats}. 


We have collected the following corpus, covering the domain of computer science, physics \& mathematics and medicine.
The statistics of these datasets are summarized in Table S1, and the details are described below.

\noindent \textbf{wiki}
already pretrained
use a subset of wikipedia


\noindent \textbf{technical domain}
fine tune on a


\noindent \textbf{Quora} We use the publicly available Quora duplicate question detection dataset \footnote{quora.com}. Each data point is a pair of plain text questions, as well as a label denoting whether they are duplicate questions, and the goal is to predict this label.

\noindent \textbf{DBPedia}
We use the DBPedia ontology dataset, release by \cite{zhang2015character}. 
It consists of 14 nonoverlapping class of entities in DBPedia, each is associated with a textual description as well as the class it belongs to, and the goal is to predict the class using the text.

\noindent \textbf{Aminer}
We collected another dataset for the classification task, by picking 14 nonoverlapping categories of computer science papers from the complete paper collection from academic search engine \cite{Tang08KDD}. Each paper is associated with title and abstract, and the goal is to predict its category.


\noindent \textbf{Arxiv CS}
We collected all the papers from arxiv.org under the category of computer science from the year of 2008 to 2018. Each paper is associated with its title and abstract. 

\noindent \textbf{Pubmed}
We construct a text collection in the medicine domain, 
select the 31 publication sources 
from the Pubmed paper abstract collection \footnote{https://www.ncbi.nlm.nih.gov/pubmed/}
that has more than 1M  bytes of words and SCI impact factor above 1. Each paper is associated with title and abstract.



\noindent \textbf{Computer Science} 
The Computer Science corpus is obtained by crawling paper abstracts from \url{arXiv.org} under the top level category "computer science", and aligning the arxiv's category with aminer.org's academic conference classification\footnote{\url{https://aminer.org/ranks/conf}}.

\noindent \textbf{Physics \& Mathematics} 
The Physics \& Mathematics corpus is also obtained from \url{arXiv.org}, 
by crawling paper abstracts under the top level category "physics" and "mathematics", and aligning them with the math subject headings\footnote{\url{https://www.maa.org/press/periodicals/loci/joma/subject-taxonomy}}
and the physics subject headings\footnote{\url{https://physh.aps.org/}},
\nop{
which results in a height 5 taxonomy tree of 80 nodes. 
We selected 573K documents that are only associated one primary category in the taxonomy tree as our corpus, which we will be able to use as automatically labeled test data.
}
\nop{
There are 67 categories in total, and we manually group them into an hierarchy according to 
This results in X documents, a height 5 taxonomy with 50 nodes total.
}

\noindent \textbf{Medicine}
The Medicine corpus is obtained by crawling abstracts from Pubmed\footnote{\url{https://www.ncbi.nlm.nih.gov/pubmed/}}, and we directly take the top 3 level of 
the Medicine subject headings\footnote{\url{https://www.nlm.nih.gov/mesh/meshhome.html}} under the top level category "Organisms", "Analytical, Diagnostic and Therapeutic Techniques, and Equipment" and "Psychiatry and Psychology" as the taxonomy.

\subsubsection{Downstream prediction}

\noindent \textbf{MNLI} Multi-Genre Natural Language Inference is a large-scale, crowdsourced entailment classification task. Given a pair of sentences, the goal is to predict whether the second sentence is an entailment, contradiction, or neutral with respect to the first one.

\noindent \textbf{QQP} Quora Question Pairs is a binary classification task where the goal is to determine if two questions asked on Quora are semantically equivalent.

\noindent \textbf{QNLI} Question Natural Language Inference is a version of the Stanford Question Answering Dataset which has been converted to a binary classification task. The positive examples are (question, sentence) pairs which do contain the correct answer, and the negative examples are (question, sentence) from the same paragraph which do not contain the answer.

\noindent \textbf{CoLA} The Corpus of Linguistic Acceptability is a binary single-sentence classification task, where the goal is to predict whether an English sentence is linguistically “acceptable” or not.

\noindent \textbf{STS-B} The Semantic Textual Similarity Bench-mark is a collection of sentence pairs drawn from news headlines and other sources. They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning.

\noindent \textbf{MRPC} Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent.

\noindent \textbf{RTE} Recognizing Textual Entailment is a binary entailment task similar to MNLI, but with much less training data.

\noindent \textbf{WNLI} Winograd NLI is a small natural language inference dataset deriving from. The GLUE webpage notes that there are issues with the construction of this dataset, and every trained system that’s been submitted to GLUE has has performed worse than the 65.1 baseline accuracy of predicting the majority class. We therefore exclude this set out of fairness to OpenAI GPT. For our GLUE submission, we always predicted the majority class.


\subsection{Performance evaluation}
We presented our experimental results below, divided by the specific tasks: predicting similarity between pair of sequences, i.e. text similarity, 
predicting label from sequence, i.e. text classification, 
and predicting the sequence itself, i.e. language modelling. 
All the code, pre-processed data and hyper-parameter configurations will be released to public.



\noindent \textbf{Predicting the similarity between pair of sequences}
We evaluate our approach over the task of predicting the similarity between pair of sequences, using the Quora dataset. The result is shown in \autoref{tab-seq-pair}, where the performance is measured using the cross entropy loss as well as prediction accuracy. Again the n-gram embedding bring benefits to the model, for example, adding n-gram embedding as word replacement to CNN architecture reduce the loss by 5.7\%.

\noindent \textbf{Predicting label from sequence}
The results for different datasets and different models are shown in \autoref{tab-seq-to-label}. For each task, we measure its performance via the cross entropy loss, as well as the prediction accuracy. We can see that adding our n-gram approach brings positive benefit to different architectures, according to both metric, with the improvement in loss function more evident. For example, adding n-gram embedding to the LSTM model reduce the loss by 20\%, and 7\%, for each respective dataset.


\begin{table*}[thb]
\centering
\small
\begin{tabular}{lcclccc} 
\toprule
Datasets / Reported Accuracy         & Accuracy & Vocab                & Methods             & AUC         & Vocab@-3\% & Vocab@-5\%  \\ 
\midrule
\multirow{4}{*}{Snips / 96.7~\cite{DBLP:conf/interspeech/LiuL16}} & 95.9       &  \multirow{4}{*}{11000} & Frequency       &  77.4    & 81         & 61          \\
                       & 95.9       &                        & TF-IDF           &   77.6   & 81           & 62          \\
                       & 95.6       &                        & Group Lasso       &   82.1   & 77           & 52\\
                       & 96.0 &                        & VVD & \textbf{82.5}  & \textbf{52}  & \textbf{36}         \\ 
\midrule
\multirow{4}{*}{ATIS-Flight / 94.1~\cite{goo2018slot}}  & 93.8       &  \multirow{4}{*}{724}    & Frequency      &  70.1   & 33           & 28    \\
                       & 93.8       &                        & TF-IDF           &  70.5  & 34          & 28\\
                       & 93.8       &                        & Group Lasso      &  72.9  & 30           & 26\\
                       & 94.0       &                        & VVD & \textbf{74.8}  & \textbf{29}  & \textbf{26}  \\
\midrule
\midrule
\multirow{5}{*}{AG-news / 91.1~\cite{zhang2015character}}     & 91.6       &     \multirow{4}{*}{61673} & Frequency    &  67.1 &    2290        & 1379\\
                             & 91.6       &                        & TF-IDF          &     67.8    &    2214     &    1303 \\

                             & 91.2       &                        & Group Lasso         &  68.3   &    1867     &  1032 \\
                             & 91.6       &                        & VVD & \textbf{70.5}  &  \textbf{1000}  & \textbf{673}   \\ 
\midrule
\multirow{4}{*}{DBPedia / 98.3~\cite{zhang2015character}}     & 98.4       &  \multirow{4}{*}{563355} & Frequency          &  69.7 &    1000    & 743            \\
                             & 98.4       &                        & TF-IDF           &   71.7    & 1703           & 804      \\

                             & 97.9       &                        & Group Lasso         &     71.9   &     768        &    678        \\
                             & 98.5       &               & VVD & \textbf{72.2}  & \textbf{427}   & \textbf{297}   \\ 
\midrule
\multirow{4}{*}{Sogou-news / 95.0~\cite{zhang2015character}}  & 93.7       & \multirow{4}{*}{254495}  & Frequency     & 70.9  & 789 & 643 \\
                             & 93.7       &                        & TF-IDF           & 71.3    & 976  &  776      \\
                             & 93.6       &                        & Group Lasso      & 73.4  &  765 &  456 \\
                             & 94.0       &                        & VVD & \textbf{75.5}  & \textbf{312}   & \textbf{196}   \\ 
\midrule
\multirow{4}{*}{Yelp-review / 58.0~\cite{zhang2015character}} & 56.3       &    \multirow{4}{*}{252712} & Frequency           &  74.0        &      1315      &     683        \\
                             & 56.3       &                        & TF-IDF           &     74.1      &     1630       &    754       \\

                             & 56.5       &                        & Group Lasso         &    75.4       &  934       &     463     \\
                             & \textbf{57.4}       &                        & VVD & \textbf{77.9}      & \textbf{487} & \textbf{287}   \\
\midrule
\midrule
\multirow{4}{*}{SNLI / 86.7~\cite{DBLP:conf/naacl/WilliamsNB18}} & 84.1     &  \multirow{4}{*}{42392} & Frequency           &   72.2   &   2139      &  1362 \\
                      & 84.1     &                        & TF-IDF              &  72.8    &    2132   &      1429 \\
                      & 84.6     &                        & Group Lasso         &  73.6    &    1712  &      1093 \\
                      & \textbf{85.5}     &                        & VVD & \textbf{75.0} & \textbf{1414} & \textbf{854}  \\
\midrule
\multirow{4}{*}{MNLI / 72.3~\cite{DBLP:conf/naacl/WilliamsNB18}} & 69.2     &  \multirow{4}{*}{100158} & Frequency           &   78.5   & 1758         &  952        \\
& 69.2     &                        & TF-IDF              &   78.7  &   1656      & 934 \\
& 70.1     &                        & Group Lasso         &   79.2    & 1466         &  711 \\
& \textbf{71.2}     &                        & VVD &  \textbf{80.1} &  \textbf{1323}  & \textbf{641} \\
\bottomrule
\end{tabular}
\caption{Experimental Results on various NLP tasks and datasets on the proposed metrics in~\autoref{sec:metrics}. Bold accuracy means the result is statistically significantly better than the competitors. }
\label{tab:result}
\end{table*}




\noindent \textbf{Predicting the sequence itself}
We apply our n-gram embedding  approach to task of predicting sequences itself, using the Arxiv CS and Pubmed dataset. To evaluate the result of n-gram embedding modification strategy, we only keep the data instance when there exists at least 1 free base entity in the text. The results are shown in \autoref{tab-seq-modeling}. We can see that adding the n-gram embedding brings clear benefit to the CNN model, reducing its perplexity on both datasets by a large margin. The results on the LSTM model varies from dataset: the loss on Arxiv CS is reduced by 10\%, but the loss on Pubmed is not reduced much, and even worse though, is that the accuracy comes down from 0.08\% to 0.06\%. 
 We attribute this to the fact that plain LSTM without attention may not fully enjoy the additional information that n-gram embedding brings to the network, and these extremely low accuracies may not be fully capture the performance of the model.





In this section, we evaluate the proposed methods with extensive experiments across several technical domains and demonstrate its efficiency and effectiveness. 


\noindent \textbf{Overall performance}
We present the results of the overall performance evaluation in \autoref{table-overall}. Hiercon achieves the best performance overall by effectively deriving concept representation for documents and hierarchy labels, 
and comprehensively utilizing all the concept similarity,
and allowing them to flow to downstream relevance computation.
Pretrained BERT also works relatively well, which confirms the validity of training label, and the generalizability of pre-trained weights; 
WeSHClass is able to improve it by efficiently utilizing training signals.
Dataless is able to achieve coarse level categorization, as seen 
by the tree accuracy, but in general suffers bad performance when  the knowledge base coverage is low.





\subsection{Qualitative analysis}

We compare the proposed vocabulary selection algorithm against several strong baselines on a wide range of text classification tasks and datasets. 

baseline: bert
bert fine tune 3

vary vocab size
vary no vb




\begin{table}
\centering

\resizebox{\columnwidth}{!}{%
\begin{tabular}{c|c|c} \hline
\multirow{2}{*}{} & \multicolumn{2}{c}{Concepts} \\ \cline{2-3} & discriminative & indiscriminative \\ \hline
\multirow{3}{*}{\it{\textbf{Computer Science}}} & \it{fpga (Hardware)} & \it{framework} \\ & \it{nash 
equilibria (Game Theory)} & \it{goal} \\ & \it{attacks (Security)} & \it{technique} \\ \hline
\multirow{3}{*}{\it{\textbf{Physics & Maths}}} & \it{entanglement (Quantum Physics)} & \it{correspondance} \\ & \it{Jupyter (Planet astrophysics)} & \it{metric} \\ & \it{complete graph  (Combinatorics)} & \it{series} \\
\hline
\multirow{3}{*}{\it{\textbf{Medicine}}} & \it{MRI (Diagnosis)} & \it{levels} \\ & \it{treatment (Therapeutics)} & \it{ability} \\ &  \it{HIV (Viruses)} & \it{regression} \\ \hline
\end{tabular}
}
\vspace{3pt}
\caption{Example discriminative vs indiscriminative concepts discovered in each dataset}
\vspace{-10pt}
\label{table-top-concepts}
\end{table}


\noindent \textbf{Discriminative \& indiscriminative concepts}
If we treat each concept as a document and perform classification on it, we can obtain the direct relevance between concepts and taxonomy nodes. \autoref{table-top-concepts}
shows some of the most discriminative \& indiscriminative concepts. We can see that the concept representation of taxonomy nodes is able to capture latent semantics and make meaningful distinctions on the concept level.







