%!TEX root =  concept_mining.tex


\nop{ refers to the idiosyncratic, non-composable, expression that span across different word, which includes
idioms such as "dog day", which me- ans "a period of inactivity", 
named entities such as "Palo Alto", 
or technical terms such as "generative adversarial network" \cite{sag2002multiword, baldwin2010multiword, constant2017multiword, hashimoto2016adaptive}. 
}

\section{Introduction}

%Multiword expressions (MWEs) are linguistic objects containing two or more words and showing some degree of non-compositionality. For instance, the meaning of to kick the bucket (i.e. ‘to die’) cannot be predicted from the meaning of its components, while the (masculine) gender of un peau rouge (’redskin’ in French) is not inherited from its nominal component (peau ’skin’ is feminine).
%MWEs encompass versatile linguistic objects: compounds (air brake), complex terms (random access memory), multiword named entities (European Bank for Reconstruction and Development), light-verb constructions (to take a nap), phrasal verbs (to make up for sth), idioms (to kick the bucket), proverbs (Fortune favours the bold.), etc Basic facts about MWEs are that: (i) they are prevalent in natural language texts – up to 40% of text items belong to MWEs (Gross and Senellart 1998, Sag et al. 2002);

The existence of atomic, non-composable Multi-word expressions (MWE) in text data has been widely recognized :  Jackendoff et.al. \cite{jackendoff1997architecture} estimates that 
the number of MWEs in a speaker's lexicon is of the same order of magnitude as the number of single words.
This is especially the case for specialized domain, such as terminology. 
Justenson et.al. \cite{justeson1995technical} claims that 
"when native English forms are used to create new terms, 
it most often takes at least two words to adequately specify a meaning", 
and even if the terminology is not in the form of MWE, they 
"are Greek or Latin forms made up of more than one root, e.g. aerodynamics". 
In \autoref{tab-examplar} we list some exemplar MWEs from the language modeling and understanding benchmarks dataset we use, 
which can be helpful in conveying meaning and determining the context 
to help support modeling and understanding text.
Driven these observations are extensive applications such as information retrieval \cite{xiong2015query, dalton2014entity, shen2018entity}, text classification \cite{furnkranz1998study}, machine translation \cite{koehn2003statistical}, parsing \cite{cafferkey2007multi, savary2015parseme} information extraction and knowledge base construction \cite{banko2007open, wu2012probase, ren2016automatic}, among many others.

%claims that our mental lexicon (vocabulary) of MWEs is as extensive as the one that is made up of single words, 

Despite the its wide existence,  multi-word expression has been largely overseen by deep learning approaches, 
whose typical processing pipeline goes like this:
given a piece of input text, 
we first tokenize it into a \textit{linear sequence representation} made up of single word tokens or word pieces,
and treat it as 1d sequential data to apply neural network models \cite{collobert2011natural, zhou2016attention, devlin2018bert}, which in the past 2 years, has converged towards the deep self attention networks \cite{vaswani2017attention}. 
All of these approaches rely on the assumption that the meaning of text can be captured by composing the meanings from its individual component words, 
and is thus contradicting with the existence of multi-word expressions, which are by definition ``non-composable".


\begin{table}
\centering
\caption{Example multi-word expression used in our }\label{tab:dataset-stats}

\begin{tabular}{ll}
\toprule
\textbf{Category}  & \textbf{Top Concepts} \\
\midrule
Arxiv CS & \multicolumn{1}{p{.35\textwidth}}{\raggedright causal models, convergence rates, decision trees, statistical inference, ensemble learning, statistical tests, support vector machines, training instances, discriminative learning, transfer learning, test set
} \\
\midrule
Wiki & \multicolumn{1}{p{.35\textwidth}}{\raggedright los angeles, new york times, civil war, virginia tech, american beauty, second world war, lock haven, liu kang, soviet union, san francisco, latin america, hong kong, grand slam, new england, mexico city, ...
}  \\
\bottomrule
RTE & \multicolumn{1}{p{.35\textwidth}}{\raggedright united states, new york, hurricane katrina, dick cheney, gulf war, atomic bomb, dow jones, gerhard schroeder, sunshine coast, game theory, big bang, tom cruise, new south wales, united nations, green card, ...
}  \\
\bottomrule
\end{tabular}
\label{tab-examplar}
\end{table}

Extending deep learning models with non-composable multi-word expressions is non-trivial. 
The reasoning is as follows. 
Firstly, the current, \textit{linear sequence representation} of text provides a clean and uniform interface to downstream neural network architecture, 
adding MWE into the representation changes the modality of the input and disrupts further architeture design. 
Furthermore, in contrast to single words,  one could generate large amount of word combinations which may or may not be real MWEs. 
Simply importing all of them into the network would introduce large amount of noise and overwhelm the self attention network with false positive information. 
How can we combine the non-composable MWE with the \textit{linear sequence representation} and best support the network design?


In this work, 
we propose \BertMWE, a generalization of deep self attention network architecture \cite{vaswani2017attention}, 
that combines the best of the original deep neural network such as Bert's ability to compose information from single words \cite{devlin2018bert} along with the additional, non-composable information from in MWE.
By dynamically adjusting the information passages in the original network and infusing it with additional non-compositionality information, 
and leveraging the efficient, variational inference techniques to uncover the underlying parameter distribution, 
\BertMWE strictly generalizes the original, transformer architecture, and fully inherits the original network's capability for modeling single word sequences, and make additional performance improvements.

We exploit the fact that the deep self-attention process the linear sequence representation of text by transforming into an unordered set, with the original ordering in the sequence encoded as position embedding,
and proprose an \textit{homogeneous injection} approach to incorporate the MWE into the network:
we feed the set of MWEs along with the original unordered set to the self attention network, with position embedding analogously.
Not only does the modality of the input stays the same and so does the self attention network architecture,
but the model for processing the original linear sequence representation still remains the same: if we dim out the effect of MWEs to zero, the original deep self-attention network is recovered.

Now the question becomes whether or not to dim out the effect of MWEs, and furthermore, which MWEs are "noise" that  we should dim out, and which MWE are crucial ones that we should not dim out?

To that end, we propose the model, \Bert-Mwe , 
which is a principled probabilistic framework  for us to reason with the original self-attention model with MWE, and selectively infer how much to “dim out” the original MWE, based on a posterior inference over the their non-composability.
by viewing the output as a mixture of all different possible latent selection on whether and how much to incorporate different MWEs into the network, and reasoning about these latent selections.
Specifically, we aim to learn a distribution over all possible models  with different degrees of non-compositionality assigned to each specific MWEs,  so that at the end of model output, the marginal likehood of the observed model output is optimized.
Then, we propose an efficient variational bayes procedure  to efficiently perform inference over the posterior model distribution, and provide an efficient implementation inside to incorporate into the state of deep  network. 

In order to 
it is critical for the model to be able to filter out noise false and capture cri, 
To that end, we propose a principled probabilistic framework for inferring the network architecture and the natural language non-compositionality, 
to dynamically add extra MWE information in a data-aware fashion
that best supports the downstream predictive performance. 
Specifically, we aim to learn a distribution over all possible model that with different degrees of non-compositionality assigned to each specific MWEs, and so that at the end of model output, the expected probability of the observed model output is optimized, averaged across all different model configurations.
We then propose an efficient variational bayes algorithm to efficiently perform inference over the posterior model distribution, and provide an efficient implementation scheme to integrate it into the state of the art deep self attention network architecture. 

We extensively evaluate the effectiveness of our approach over a wide range of tasks for language modeling and understanding, and demonstrate that \BertMWE consistently preserves and improves upon the state of the art baselines approaches such as the Bert model \cite{devlin2018bert}.



The rest of the paper is organized as follows. In section 2 we provide a comprehensive review of previous work and further motivate our approach. In section 3 we discuss the preprocessing steps needed, and in section 4 we discuss our approach to incorporate the n-gram into deep learning model. We present experiment results in Section 5 and draw conclusion in Section 6.
