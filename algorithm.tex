\subsection{Efficient approximate inference}\label{sec:approximate-inference}

In order to evaluate the derivative and optimize the ELBO (see \autoref{eq:elbo0}) without exact integration over the latent variables, 
we resort to sampling based methods 
that approximate the derivative value by drawing random samples from the variational distribution \cite{ranganath2014black}. 
However, directly sampling from the distribution, e.g. through stochastic gradient variational bayes  \cite{rezende2014stochastic} might be hard due to high variance in the estimated gradients and the sheer amount of latent variables \cite{kingma2015variational}.
We instead leverage the local re-parameterization trick \citep{Kingma2015} to  
decompose the global sample uncertainty in the model weights into local noise, and remove the correlation between samples in the mini-batch.

Another challenge is that, because the generation process follows a Bernouli distribution, 
which is discrete and thus block the back-propagation from the estimated likelihood, 
it is very hard for us to obtain gradients about the variational parameters and re-adjust the distribution. 
One possible approach is to relax the discrete distribution into a continuous, Gaussian distribution \cite{wang2013fast, chen2019large}. However, these two families of distributions are of very different nature and 
requires significant hand-tuning such as truncating in order  to prevent the computation from diverging, and in practice cause the model to underperform \cite{molchanov2016dropout}, not to mention that fact the computation error could accumulate as the number of layers increases, and more  approximation is applied to each layer. 

To remedy this, we adopt the Gumbel-Max trick \cite{gumbel1954statistical, gummaddison2014sampling} as the continuous relaxation of the Bernoulli distribution
and 
make the sample differentiable with respect to the variational parameter. 
Specifically, the continuous relaxation to Bernoulli distribution with mean parameters $p$ can be parameterized as
\begin{equation}
Gumble_{t}( \tilde{\beta} | p) = t \left( \frac{p}{\tilde{\beta}^t} + \frac{1 - p}{(1- \tilde{\beta})^t} \right)^{-2} 
\frac{p}{\tilde{\beta}^{(t + 1)}} \frac{1 - p}{(1 - \tilde{\beta})^{(t + 1)}}  
\\
\end{equation}
where $t$ is the "temperature" parameter that controls how close the continuous relaxation is to the Bernoulli distribution: when it approach 0, the resulted $y$ recovers the original Bernoulli distribution.

For Bernoulli random variable with mean parameter $\pi$, this amounts to drawing a uniform random variable  $u \sim \Unif (0, 1)$ and compute the sampled value $z$
 through a sigmoid function \cite{gal2017concrete}
\begin{align}
\tilde{\beta} &= \text{sigmoid} \bigg(
\frac{1}{t} \cdot \big(
\log \pi
- \log (1 - \pi)
+ \log u
- \log (1 - u)
\big)
\bigg) \label{eq:gumbel-sample}
\end{align}


Without loss of generality, we consider the case where the discrete variables $B$ and $\{
\beta_i | i \in V\}$ are directly replaced with Gumbel-softmax distributed continuous relaxation counterpart, denoted $\tilde{B}$ and $ \{ \tilde{\beta}_i  | i \in V\}$, and 
we apply these relaxed quantities multiplicatively into the model weights for the corresponding MWEs in $\Theta$, 
instead of multiplying them by a hard 0, 1 as in the original Bernoulli distribution case.
Denoting $ \tilde{Q} (\Theta | W, \Pi, \{\pi_i | i \in V\})$ as the re-parameterized variational distribution, 
the expected conditional log likelihood for the observed data $\mathcalD$ can then be re-written as
\begin{align}
& \mathbb{E}_{\tilde{\Theta} \sim \tilde{Q}} \log P (\mathcalD | \tilde{\Theta}) \nonumber\\ 
= & \sum_{x, y \in \mathcalD} \int_{\tilde{B}} \int_{\tilde{\beta}_i}\int_{\Theta} Gumble(\tilde{B} | \Pi) \prod_i^{V} \log ( Gumble(\tilde{\beta}_i | \pi_i) \nonumber\\ 
& \quad \quad \quad \quad p(\Theta \vert W, B, \{\beta_i, i \in V\}) \nonumber\\ 
& \quad \quad \quad \quad p(y \vert f^{\Theta}(x)) )
\label{eq:exp-gumble}
\end{align}

Our goal now becomes, to optimize the evidence lower bound indicated by the above expected conditional log likelihood 

\begin{align}
    &\log P( \mathcalD ) \nonumber\\
    \geq& \mathbb{E}_{\tilde{\Theta} \sim \tilde{Q} } \log P (\mathcalD | \tilde{B}, \tilde{\Theta})  - D_{KL} ( \tilde{Q} ( \tilde{\Theta}) ||\mathcal{P}( \tilde{\Theta} )) \nonumber\\
    = & \mathcal{L}_{Gumble} (W, B, \{\beta_i, i \in V\}) \label{eq:elbo0}
\end{align}

The first term in the ELBO refers to the expected conditional log likelihood of the data, as defined in \autoref{eq:exp-gumble}, 
and the second term is the KL divergence between the new variational distribution $\tilde{Q} ( \tilde{\Theta})$ 
and a predefined prior distribution  $\mathcal{P}( \tilde{\Theta})$.
Although we can obtain its value via analytical formula or Monte Carlo sampling \cite{gal2016uncertainty}, 
we ignore this term in practice because it is of $O(N)$ orders of magnitude smaller than the expected conditional likelihood, with $N$ being the size of data to train the model. 
This is especially true when we can train the model's weights using very large amount of training instances, e.g. with the language modeling objective \cite{devlin2018bert}.





\begin{figure}[htb]
    \centering
    \includegraphics[width=0.95\linewidth]{fig/model_architecture.png}
    \vspace{20pt}
    \caption{Illustration of the \BertMWE network architecture. 
The \textit{MWE Dropout} is repetitively applied to the input vector representations of each self attention layer. 
If dropout out ratio is zero, the entire network architecture corresponding to that MWE unit will be dimmed out, 
effectively reducing network back to the original structure without that MWE. On the other hand, if the dropout ratio is one, 
 the effect of that MWE will be completely preserved as if not no dropout is applied.
}
    \vspace{10pt}
    \label{fig:architecture-mwe-dropout}
\end{figure}



\subsection{Network architecture}
Finally, we describe the details of the network architecture design to fully specify the model parameters. 
The challenge is that, by relaxing the Bernoulli distribution to the continuous Gumbel-Max, we can no longer treat the selection of MWEs as black or white decisions
and the non-compositionality inference scheme needs to 
be dynamically integrated into the model architecture at multiple levels. 

Our proposed architecture is shown in \autoref{fig:architecture-mwe-dropout}. 
Given one instance of input text, the first step is to pass the text into the \textit{Input Embedding} layer to transform the tokens into vector representations to feed into downstream neural networks.
For single word embedding, this amounts to combining the token embedding, position embedding and sentence level information altogether into a set of vector representation
corresponding to each token. For this we follow the exact original implementation as in \cite{devlin2018bert}.

In addition to embedding single words, 
we also pass the input into \textit{MWE embedding}, and similarly generate a set of vector representation 
by following the procedure described in  \autoref{sec:arch-overview}, which is passed through the \textit{Concat} layer to be 
concatenated with the vector representations from single words.

In addition, according to the variational inference scheme we specified \autoref{sec:arch-overview}, 
we will also generate the 
sampled variational parameters $B, \{\beta_i\}$ for each the corresponding MWE, based on the their priors $\{\pi_i \}$. 

We then apply $B, \{\beta_i\}$ to their associated MWEs. Simply applying them to the input vector representations of the MWEs will not suffice. 
This is because 
the partially dimmed out MWE embedding will affect downstream network architectures beyond the input level, which may cause the network to eventually diverge from the desired output.

%that dropout the a set of related weights across the entire neural networks, simultaneously controlled by the same sampled dropout ratio.


To address this, we propose a novel approach for applying these variational parameters, called "deep dropout", that not only apply the dimming effect on the input level, but also repetitively 
at the corresponding positions for the downstream network levels, simultaneously controlled by the same "dimming ratio".

Specifically, we apply a \textit{MWE Dropout} to 
the input vector representations of each network layer, before any further operation, where the variational parameter $B, \{\beta_i\}$ are used as 
the ``dropout" ratio to be applied multiplicatively into the vector representations.
If we denote the input vector representation as a matrix $\mathbb{E}$, and the corresponding dropout ratio as $\vec{\beta}$, 
the output of the \textit{MWE Dropout} $f(\mathbb{E}, \vec{\beta})$ can be specified as 
\begin{equation} \label{eq:MWEDropout}
   f(\mathbb{E}, \vec{\beta}) =  diag(\bm{\beta}) \cdot \mathbb{E}
\end{equation}
By applying the MWE-Dropout layer before every operation in the same self attention layer, including the 
\textit{Multi-Head Attention}, \textit{Add & Norm}, \textit{FeedForward} and another \textit{Add & Norm}, layer, as in \cite{vaswani2017attention}, 
we are effectively removing the MWE embedding by the  desired ratio:
If dropout out ratio is zero, the entire network architecture corresponding to that MWE unit will be dimmed out from each layer of the network,
effectively reducing the network back to the original structure without that MWE;
If the dropout ratio is one, the effect of that MWE will be completely preserved as if there is no dropout is applied.
If the ratio is between zero and one, 
the dimming effect will be applied repetitively to all network layers, jointly changing the behavior of the entire network.

This newly grouped self attention block, consisting of the original self attention layer as well as the \textit{MWE Dropout} layer, is then stacked repeatedly for $N$ times, and finally fed into the donwstream task-specific prediction heads, also following the original implementation \cite{devlin2018bert}.


\subsection{Posterior Predictive Inference} \label{sec:prediction}
The outcome of the variational inference is the set of parameters governing the variational distribution, and the now the task is to obtain the model prediction for a given input $x$.
Instead of maximizing the variational distribution to predict a point estimate, i.e. maximum a priori, we perform Posterior Predictive Inference that comprehensively 
consider the entire posterior predictive distribution in making the predictions.

Specifically, the posterior predictive distribution of $y$ under the variational distribution $\tilde{Q}$ can be obtained by computing the expectation of the model parameters $\tilde{\Theta}$ under all possible model settings
\begin{align}
& \mathbb{E}_{\tilde{\Theta} \sim \tilde{Q}} P (y | x, \tilde{\Theta}) \nonumber\\ 
= & \int_{\tilde{B}} \int_{\tilde{\beta}_i}\int_{\Theta} Gumble(\tilde{B} | \Pi) \prod_i^{V} Gumble(\tilde{\beta}_i | \pi_i) \nonumber\\ 
& \quad \quad \quad \quad \quad \quad \quad \quad p(\Theta \vert W, B, \{\beta_i, i \in V\}) \nonumber\\ 
& \quad \quad \quad \quad \quad \quad \quad \quad p(y \vert f^{\Theta}(x)) 
\label{eq:prediction}
\end{align}

We provide an efficient sampling scheme:
at test time,
we use the exact same Gumble-softmax sampling operation (\autoref{eq:gumbel-sample}) and keep the same behavior for the \textit{MWE Dropout} layer (\autoref{eq:MWEDropout}) as in the training stage to pass the input through, and use the model output as the sample from the posterior predictive distribution.